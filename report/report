run11:

  trying to stop training d if g gets to bad. staying with adam for now


  #################################################################################
  # Globals
  #################################################################################

  RUN = 11

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( Adam(lr=0.000002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( Adam(lr=0.00008, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( Adam(lr=0.00008, clipvalue=1.0, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )

  g_acc_history = 10


  Epoch: 0;  D loss: 0.499999988824, acc: 3.93696565926; G loss: 9.55114108324, acc: 0.0
Epoch: 10;  D loss: 0.519999982789, acc: 0.681199535728; G loss: 3.64391556382, acc: 0.0
Epoch: 20;  D loss: 0.664999984205, acc: 0.484216069803; G loss: 2.36355373263, acc: 0.0
Epoch: 30;  D loss: 0.852499973029, acc: 0.414964782074; G loss: 1.51079817116, acc: 0.0
Epoch: 40;  D loss: 0.884999971837, acc: 0.482505854219; G loss: 0.879903912544, acc: 0.22499999404
Epoch: 50;  D loss: 0.572499979287, acc: 0.673770837486; G loss: 0.533446785063, acc: 0.789999969304
Epoch: 60;  D loss: 0.329999992624, acc: 0.980017963797; G loss: 0.573702860624, acc: 0.669999971986
Epoch: 70;  D loss: 0.284999990836, acc: 1.28948437423; G loss: 1.49363780767, acc: 0.524999991059
Epoch: 80;  D loss: 0.274999991991, acc: 2.68823380023; G loss: 3.50836212933, acc: 0.514999978244
Epoch: 90;  D loss: 0.264999994077, acc: 3.6110175997; G loss: 3.50940099359, acc: 0.429999984801
Epoch: 100;  D loss: 0.214999990538, acc: 4.53671370447; G loss: 3.65380141139, acc: 0.399999994785
Epoch: 110;  D loss: 0.287499993108, acc: 3.08704514056; G loss: 5.89460676908, acc: 0.33499999065
Epoch: 120;  D loss: 0.259999990463, acc: 3.20342101157; G loss: 5.33400726318, acc: 0.314999992959
Epoch: 130;  D loss: 0.389999986626, acc: 2.44024128467; G loss: 11.7791399956, acc: 0.11499999743
Epoch: 140;  D loss: 0.417499987409, acc: 1.61583415046; G loss: 11.109869957, acc: 0.139999994077
Epoch: 150;  D loss: 0.452499982901, acc: 1.39554917067; G loss: 13.070858717, acc: 0.089999997057
Epoch: 160;  D loss: 0.47749998793, acc: 1.01026613265; G loss: 13.6940602064, acc: 0.0599999986589
Epoch: 170;  D loss: 0.477499991655, acc: 0.895996890962; G loss: 14.7178217173, acc: 0.0399999991059
Epoch: 180;  D loss: 0.509999977425, acc: 0.908760832623; G loss: 14.5462334156, acc: 0.0349999992177
Epoch: 190;  D loss: 0.517499981448, acc: 0.682497020811; G loss: 15.1128629446, acc: 0.0299999993294
Epoch: 200;  D loss: 0.534999990836, acc: 0.602501347661; G loss: 14.8478428125, acc: 0.0349999992177
Epoch: 210;  D loss: 0.549999980256, acc: 0.68774230592; G loss: 13.1545995474, acc: 0.0949999969453
Epoch: 220;  D loss: 0.559999974445, acc: 0.865797892213; G loss: 13.0392212868, acc: 0.0749999983236
Epoch: 230;  D loss: 0.519999980927, acc: 0.663398150355; G loss: 12.3457890749, acc: 0.119999998249
Epoch: 240;  D loss: 0.352499989793, acc: 1.57801526412; G loss: 5.09816592932, acc: 0.3399999924
Epoch: 250;  D loss: 0.279999993276, acc: 2.19754514843; G loss: 2.92763134837, acc: 0.524999987334
Epoch: 260;  D loss: 0.167499993462, acc: 3.09241048247; G loss: 3.38781873882, acc: 0.434999994002
Epoch: 270;  D loss: 0.179999995045, acc: 4.5269241631; G loss: 3.10087494552, acc: 0.449999980628
Epoch: 280;  D loss: 0.24749999214, acc: 4.82904553413; G loss: 4.8082253933, acc: 0.374999986961
Epoch: 290;  D loss: 0.0849999985658, acc: 8.06691277027; G loss: 1.15578332171, acc: 0.329999985173
Epoch: 300;  D loss: 0.0399999991059, acc: 8.07499212027; G loss: 0.811925170012, acc: 0.209999994375
Epoch: 310;  D loss: 0.019999999553, acc: 8.17062357068; G loss: 1.18211358786, acc: 0.119999996386
Epoch: 320;  D loss: 0.0374999991618, acc: 8.06823348999; G loss: 0.523094683886, acc: 0.149999993853
Epoch: 330;  D loss: 0.0974999978207, acc: 7.21632540226; G loss: 2.68053363264, acc: 0.244999990799
Epoch: 340;  D loss: 0.139999996405, acc: 6.47762742639; G loss: 3.56956043839, acc: 0.26499998942
Epoch: 350;  D loss: 0.257499991916, acc: 4.95239813626; G loss: 6.73232662678, acc: 0.179999995045
Epoch: 360;  D loss: 0.39499998372, acc: 3.29366482794; G loss: 10.9028265476, acc: 0.104999996722
Epoch: 370;  D loss: 0.434999988414, acc: 2.40970055386; G loss: 12.1938399076, acc: 0.0749999973923
Epoch: 380;  D loss: 0.382499984466, acc: 2.89620494097; G loss: 10.9878928661, acc: 0.154999997467
Epoch: 390;  D loss: 0.392499985173, acc: 2.25898274034; G loss: 11.0845352411, acc: 0.119999996386
Epoch: 400;  D loss: 0.374999988824, acc: 2.30580052733; G loss: 11.0841201544, acc: 0.109999997541
Epoch: 410;  D loss: 0.417499983683, acc: 1.9055102542; G loss: 12.2469484806, acc: 0.104999996722
Epoch: 420;  D loss: 0.429999984801, acc: 1.43598198891; G loss: 13.4006642103, acc: 0.0699999984354
Epoch: 430;  D loss: 0.427499989048, acc: 1.78742296249; G loss: 12.8549787998, acc: 0.0749999983236
Epoch: 440;  D loss: 0.457499982789, acc: 1.32225688919; G loss: 13.8352229595, acc: 0.0549999987707
Epoch: 450;  D loss: 0.442499991506, acc: 1.63590215519; G loss: 13.701572299, acc: 0.0599999986589
Epoch: 460;  D loss: 0.462499978021, acc: 1.45139551908; G loss: 13.7370197773, acc: 0.0499999988824
Epoch: 470;  D loss: 0.462499989197, acc: 1.35256707668; G loss: 13.7154700756, acc: 0.0749999983236
Epoch: 480;  D loss: 0.469999989495, acc: 1.25862365589; G loss: 14.5697302818, acc: 0.0349999992177
Epoch: 490;  D loss: 0.479999989271, acc: 1.29312666878; G loss: 13.6727410555, acc: 0.0649999985471
Epoch: 500;  D loss: 0.439999982715, acc: 1.68960439786; G loss: 12.6074424982, acc: 0.129999997094
Epoch: 510;  D loss: 0.354999986012, acc: 1.97381842881; G loss: 6.05013993382, acc: 0.279999996535
Epoch: 520;  D loss: 0.207499996293, acc: 3.37278355658; G loss: 3.14737056196, acc: 0.474999979138
Epoch: 530;  D loss: 0.169999995735, acc: 3.20095161349; G loss: 2.11378365755, acc: 0.534999988973
Epoch: 540;  D loss: 0.192499994766, acc: 2.93398685753; G loss: 1.99982883036, acc: 0.484999995679
Epoch: 550;  D loss: 0.229999993462, acc: 2.81220673025; G loss: 2.04710066319, acc: 0.459999989718
Epoch: 560;  D loss: 0.25999999186, acc: 2.6451869458; G loss: 2.30764423311, acc: 0.459999989718
Epoch: 570;  D loss: 0.214999991935, acc: 2.46885617822; G loss: 1.77622748166, acc: 0.469999976456
Epoch: 580;  D loss: 0.172499996144, acc: 2.73609046638; G loss: 1.50487151742, acc: 0.57999996841
Epoch: 590;  D loss: 0.329999987036, acc: 2.45109500736; G loss: 3.40208938718, acc: 0.329999990761
Epoch: 600;  D loss: 0.167499995325, acc: 2.87024541944; G loss: 1.34672512859, acc: 0.559999976307
Epoch: 610;  D loss: 0.272499992047, acc: 2.71339455247; G loss: 2.77954846621, acc: 0.474999990314
Epoch: 620;  D loss: 0.292499986477, acc: 2.52516842633; G loss: 2.91009344161, acc: 0.394999992102
Epoch: 630;  D loss: 0.297499989159, acc: 2.6237892434; G loss: 3.21479946375, acc: 0.359999991953
Epoch: 640;  D loss: 0.284999991767, acc: 3.25009604543; G loss: 3.41175958514, acc: 0.404999990016
Epoch: 650;  D loss: 0.38249998726, acc: 2.81771776825; G loss: 4.18399816751, acc: 0.324999988079
Epoch: 660;  D loss: 0.252499990165, acc: 2.59807182848; G loss: 2.68334139884, acc: 0.409999988973
Epoch: 670;  D loss: 0.179999995511, acc: 2.70186811686; G loss: 2.3817620948, acc: 0.494999982417
Epoch: 680;  D loss: 0.272499987856, acc: 2.45242999494; G loss: 2.83778874576, acc: 0.389999993145
Epoch: 690;  D loss: 0.289999986999, acc: 2.43285075575; G loss: 4.36895489693, acc: 0.244999986142
Epoch: 700;  D loss: 0.24999998929, acc: 2.39586150646; G loss: 2.09410466254, acc: 0.424999989569
Epoch: 710;  D loss: 0.284999988973, acc: 2.0095814243; G loss: 3.36480751634, acc: 0.3399999924
Epoch: 720;  D loss: 0.184999994002, acc: 2.60129162669; G loss: 2.2161064446, acc: 0.509999979287
Epoch: 730;  D loss: 0.212499993853, acc: 2.59924307466; G loss: 2.21124640107, acc: 0.534999981523
Epoch: 740;  D loss: 0.234999991953, acc: 2.32944433391; G loss: 2.4846054092, acc: 0.399999983609
Epoch: 750;  D loss: 0.127499995753, acc: 2.8897575289; G loss: 1.30136273056, acc: 0.594999969006
Epoch: 760;  D loss: 0.239999991376, acc: 2.00339933485; G loss: 3.02119153738, acc: 0.364999985322
Epoch: 770;  D loss: 0.309999986086, acc: 1.96107193828; G loss: 2.64274579287, acc: 0.399999989197
Epoch: 780;  D loss: 0.264999991283, acc: 1.90929108113; G loss: 2.37959526479, acc: 0.399999991059
Epoch: 790;  D loss: 0.267499990761, acc: 1.99530363828; G loss: 3.54376167059, acc: 0.299999993294
Epoch: 800;  D loss: 0.222499993164, acc: 1.95461524278; G loss: 2.13266485929, acc: 0.324999991804
Epoch: 810;  D loss: 0.247499991208, acc: 1.5959546864; G loss: 2.30971506238, acc: 0.399999991059
Epoch: 820;  D loss: 0.252499991562, acc: 1.6835147813; G loss: 2.4221932143, acc: 0.349999982864
Epoch: 830;  D loss: 0.234999994282, acc: 1.74077881873; G loss: 1.58411455154, acc: 0.429999992251
Epoch: 840;  D loss: 0.229999994393, acc: 1.88617250323; G loss: 1.7453533411, acc: 0.389999985695
Epoch: 850;  D loss: 0.292499992065, acc: 1.60714199394; G loss: 2.47293420136, acc: 0.289999986067
Epoch: 860;  D loss: 0.294999989681, acc: 1.58874294907; G loss: 2.63880079985, acc: 0.299999990501
Epoch: 870;  D loss: 0.254999991041, acc: 1.6369131431; G loss: 2.07824648172, acc: 0.36499998346
Epoch: 880;  D loss: 0.274999988265, acc: 1.49475579336; G loss: 1.79490771145, acc: 0.404999980703
Epoch: 890;  D loss: 0.182499994058, acc: 1.63275880367; G loss: 1.26929344237, acc: 0.519999980927
Epoch: 900;  D loss: 0.237499990966, acc: 1.49405933172; G loss: 2.3418610245, acc: 0.329999988899
Epoch: 910;  D loss: 0.304999992717, acc: 1.36495936662; G loss: 2.35811175406, acc: 0.224999995902
Epoch: 920;  D loss: 0.28249999322, acc: 1.30643901601; G loss: 1.04519034922, acc: 0.344999987632
Epoch: 930;  D loss: 0.234999990556, acc: 1.36511702836; G loss: 1.50670953095, acc: 0.409999985248
Epoch: 940;  D loss: 0.277499991003, acc: 1.37112681568; G loss: 1.94634387642, acc: 0.274999993853
Epoch: 950;  D loss: 0.369999990799, acc: 1.40285259113; G loss: 2.41618192196, acc: 0.154999995604
Epoch: 960;  D loss: 0.307499988936, acc: 1.18732479215; G loss: 1.15031629801, acc: 0.284999992698
Epoch: 970;  D loss: 0.289999991655, acc: 1.21709272265; G loss: 1.49583750963, acc: 0.244999992661
Epoch: 980;  D loss: 0.32749998942, acc: 1.18914267048; G loss: 1.30466125906, acc: 0.269999993965
Epoch: 990;  D loss: 0.277499993332, acc: 1.22376811504; G loss: 1.21275795996, acc: 0.294999990612
Epoch: 1000;  D loss: 0.292499988806, acc: 1.14331926033; G loss: 1.20535576344, acc: 0.324999991804
Epoch: 1010;  D loss: 0.319999989122, acc: 1.15495133027; G loss: 1.41521385312, acc: 0.264999991283
Epoch: 1020;  D loss: 0.312499989755, acc: 1.12781914324; G loss: 1.17194248736, acc: 0.204999993555
Epoch: 1030;  D loss: 0.32749998942, acc: 1.14168477058; G loss: 1.16242819279, acc: 0.269999991171
Epoch: 1040;  D loss: 0.417499983683, acc: 1.0981766358; G loss: 2.05760203302, acc: 0.10999999661
Epoch: 1050;  D loss: 0.369999986142, acc: 1.06544869766; G loss: 1.14770333469, acc: 0.194999990985
Epoch: 1060;  D loss: 0.26499999268, acc: 1.15261084586; G loss: 1.180218786, acc: 0.224999992177
Epoch: 1070;  D loss: 0.334999985993, acc: 1.10608024895; G loss: 1.43190268427, acc: 0.204999992624
Epoch: 1080;  D loss: 0.402499981225, acc: 1.05896046758; G loss: 1.71646343172, acc: 0.119999996386
Epoch: 1090;  D loss: 0.349999985658, acc: 1.05324150994; G loss: 1.11761321872, acc: 0.219999994151
Epoch: 1100;  D loss: 0.314999984577, acc: 1.04016960785; G loss: 1.18611085415, acc: 0.179999992251
Epoch: 1110;  D loss: 0.369999987073, acc: 0.982017062604; G loss: 1.15304139256, acc: 0.194999992847
Epoch: 1120;  D loss: 0.384999989532, acc: 0.99792965129; G loss: 1.26774813235, acc: 0.204999992624
Epoch: 1130;  D loss: 0.382499990053, acc: 0.951714735478; G loss: 1.30148346722, acc: 0.15999999363
Epoch: 1140;  D loss: 0.374999986961, acc: 0.95626783371; G loss: 1.14916468412, acc: 0.169999992475
Epoch: 1150;  D loss: 0.454999983311, acc: 0.886513963342; G loss: 1.33831115067, acc: 0.0599999986589
Epoch: 1160;  D loss: 0.379999992438, acc: 0.919350974262; G loss: 1.09180626273, acc: 0.159999995492
Epoch: 1170;  D loss: 0.377499988303, acc: 0.879614386708; G loss: 1.18805003911, acc: 0.10499999579
Epoch: 1180;  D loss: 0.377499991097, acc: 0.893936824054; G loss: 1.13568843901, acc: 0.119999997318
Epoch: 1190;  D loss: 0.402499985881, acc: 0.866946406662; G loss: 1.17599147558, acc: 0.124999995343
Epoch: 1200;  D loss: 0.424999987707, acc: 0.832725543529; G loss: 1.08650621772, acc: 0.119999996386
Epoch: 1210;  D loss: 0.412499981932, acc: 0.838379003108; G loss: 1.084006235, acc: 0.124999996275
Epoch: 1220;  D loss: 0.419999981299, acc: 0.796682909131; G loss: 1.03565942496, acc: 0.1299999943
Epoch: 1230;  D loss: 0.412499988452, acc: 0.789243869483; G loss: 0.931866340339, acc: 0.209999993443
Epoch: 1240;  D loss: 0.422499988228, acc: 0.769411634654; G loss: 1.0604820177, acc: 0.184999994002
Epoch: 1250;  D loss: 0.419999977574, acc: 0.771433416754; G loss: 0.990066856146, acc: 0.124999998137
Epoch: 1260;  D loss: 0.407499987632, acc: 0.76873292774; G loss: 0.939523845911, acc: 0.204999994487
Epoch: 1270;  D loss: 0.404999989085, acc: 0.76532221213; G loss: 0.915224798024, acc: 0.209999994375
Epoch: 1280;  D loss: 0.454999987036, acc: 0.737315770239; G loss: 0.88983990252, acc: 0.204999993555
Epoch: 1290;  D loss: 0.449999987148, acc: 0.743341460824; G loss: 0.888979874551, acc: 0.18499999214
Epoch: 1300;  D loss: 0.454999985173, acc: 0.72823722288; G loss: 0.860149048269, acc: 0.224999990314
Epoch: 1310;  D loss: 0.484999988228, acc: 0.721722152084; G loss: 0.822785519063, acc: 0.254999991506
Epoch: 1320;  D loss: 0.48999999091, acc: 0.710434809327; G loss: 0.835214577615, acc: 0.294999992475
Epoch: 1330;  D loss: 0.462499980815, acc: 0.721994813532; G loss: 0.80674815923, acc: 0.344999987632
Epoch: 1340;  D loss: 0.482499979436, acc: 0.719327207655; G loss: 0.787465788424, acc: 0.36499999091
Epoch: 1350;  D loss: 0.50999998115, acc: 0.697098389268; G loss: 0.778847195208, acc: 0.324999986216
Epoch: 1360;  D loss: 0.477499989793, acc: 0.70963986963; G loss: 0.76589679718, acc: 0.384999990463
Epoch: 1370;  D loss: 0.544999986887, acc: 0.693876557052; G loss: 0.782379835844, acc: 0.349999992177
Epoch: 1380;  D loss: 0.514999987558, acc: 0.697805155069; G loss: 0.766249373555, acc: 0.379999991506
Epoch: 1390;  D loss: 0.524999988265, acc: 0.701446902007; G loss: 0.786469757557, acc: 0.36999999173
Epoch: 1400;  D loss: 0.614999968559, acc: 0.676249776036; G loss: 0.788181163371, acc: 0.329999988899
Epoch: 1410;  D loss: 0.517499981448, acc: 0.694977726787; G loss: 0.780657067895, acc: 0.344999989495
Epoch: 1420;  D loss: 0.612499983981, acc: 0.651870343834; G loss: 0.770652152598, acc: 0.369999976829
Epoch: 1430;  D loss: 0.614999989048, acc: 0.653232254088; G loss: 0.781829185784, acc: 0.359999984503
Epoch: 1440;  D loss: 0.602499993518, acc: 0.67019649595; G loss: 0.776198521256, acc: 0.39499999024
Epoch: 1450;  D loss: 0.592499975115, acc: 0.672284640372; G loss: 0.759330265224, acc: 0.419999983162
Epoch: 1460;  D loss: 0.537499984726, acc: 0.697861228138; G loss: 0.760826691985, acc: 0.404999980703
Epoch: 1470;  D loss: 0.582499984652, acc: 0.674582228065; G loss: 0.735592789948, acc: 0.474999979138
Epoch: 1480;  D loss: 0.539999985136, acc: 0.686134751886; G loss: 0.740422599018, acc: 0.454999983311
Epoch: 1490;  D loss: 0.524999985471, acc: 0.700774651021; G loss: 0.766012519598, acc: 0.394999988377
Epoch: 1500;  D loss: 0.539999986067, acc: 0.691599000245; G loss: 0.711949430406, acc: 0.499999981374
Epoch: 1510;  D loss: 0.407499983907, acc: 0.749606981874; G loss: 0.684547901154, acc: 0.534999988973
Epoch: 1520;  D loss: 0.509999986738, acc: 0.728727445006; G loss: 0.73779541254, acc: 0.474999986589
Epoch: 1530;  D loss: 0.529999990016, acc: 0.708708140999; G loss: 0.84553834796, acc: 0.269999994896
Epoch: 1540;  D loss: 0.417499981821, acc: 0.745431300253; G loss: 0.79274828732, acc: 0.434999991208
Epoch: 1550;  D loss: 0.502499978989, acc: 0.716007549316; G loss: 0.730280190706, acc: 0.424999985844
Epoch: 1560;  D loss: 0.447499986738, acc: 0.736037116498; G loss: 0.726360589266, acc: 0.499999985099
Epoch: 1570;  D loss: 0.434999981895, acc: 0.729148477316; G loss: 0.697798624635, acc: 0.499999977648
Epoch: 1580;  D loss: 0.457499982789, acc: 0.717811089009; G loss: 0.752276085317, acc: 0.394999992102
Epoch: 1590;  D loss: 0.47749998793, acc: 0.712470315397; G loss: 0.74863242358, acc: 0.379999984056
Epoch: 1600;  D loss: 0.504999984987, acc: 0.699114024639; G loss: 0.772971652448, acc: 0.394999980927
Epoch: 1610;  D loss: 0.562499977648, acc: 0.692521911114; G loss: 0.807618290186, acc: 0.339999983087
Epoch: 1620;  D loss: 0.607499986887, acc: 0.659620396793; G loss: 0.786570847034, acc: 0.359999991953
Epoch: 1630;  D loss: 0.56749997288, acc: 0.680164456367; G loss: 0.796054728329, acc: 0.299999985844
Epoch: 1640;  D loss: 0.412499990314, acc: 0.749897629023; G loss: 0.789044134319, acc: 0.334999987856
Epoch: 1650;  D loss: 0.499999983236, acc: 0.704287271947; G loss: 0.743227958679, acc: 0.419999994338
Epoch: 1660;  D loss: 0.512499986216, acc: 0.688148498535; G loss: 0.736823447049, acc: 0.439999990165
Epoch: 1670;  D loss: 0.524999985471, acc: 0.693309165537; G loss: 0.724752925336, acc: 0.469999983907
Epoch: 1680;  D loss: 0.514999978244, acc: 0.706646598876; G loss: 0.740183331072, acc: 0.439999982715
Epoch: 1690;  D loss: 0.539999980479, acc: 0.695444580168; G loss: 0.763740114868, acc: 0.369999997318
Epoch: 1700;  D loss: 0.579999985173, acc: 0.685381244868; G loss: 0.736420080066, acc: 0.454999979585
Epoch: 1710;  D loss: 0.59749998711, acc: 0.674207273871; G loss: 0.696131370962, acc: 0.554999984801
Epoch: 1720;  D loss: 0.594999982044, acc: 0.65787588805; G loss: 0.712148718536, acc: 0.48999999091
Epoch: 1730;  D loss: 0.649999981746, acc: 0.63730071485; G loss: 0.655252322555, acc: 0.60499997437
Epoch: 1740;  D loss: 0.632499985397, acc: 0.647298526019; G loss: 0.653601706028, acc: 0.569999977946
Epoch: 1750;  D loss: 0.637499976903, acc: 0.642633195966; G loss: 0.655467242002, acc: 0.604999989271
Epoch: 1760;  D loss: 0.667499978095, acc: 0.624107904732; G loss: 0.599810451269, acc: 0.67999997735
Epoch: 1770;  D loss: 0.512499988079, acc: 0.698420729488; G loss: 0.596755892038, acc: 0.684999987483
Epoch: 1780;  D loss: 0.552499976009, acc: 0.691779389977; G loss: 0.618240736425, acc: 0.709999971092
Epoch: 1790;  D loss: 0.507499992847, acc: 0.703924968839; G loss: 0.590724918991, acc: 0.744999974966
Epoch: 1800;  D loss: 0.602499986067, acc: 0.649135958403; G loss: 0.696996532381, acc: 0.56499998644
Epoch: 1810;  D loss: 0.602499976754, acc: 0.657542992383; G loss: 0.588165938854, acc: 0.689999978989
Epoch: 1820;  D loss: 0.597499983385, acc: 0.661563456059; G loss: 0.631886258721, acc: 0.644999988377
Epoch: 1830;  D loss: 0.612499978393, acc: 0.667856577784; G loss: 0.609354354441, acc: 0.659999966621
Epoch: 1840;  D loss: 0.682499982417, acc: 0.609630614519; G loss: 0.618317998946, acc: 0.599999982864
Epoch: 1850;  D loss: 0.632499977946, acc: 0.635847907513; G loss: 0.599514454603, acc: 0.604999966919
Epoch: 1860;  D loss: 0.45249998942, acc: 0.746473375708; G loss: 0.659429676831, acc: 0.579999979585
Epoch: 1870;  D loss: 0.387499986216, acc: 0.756308078766; G loss: 0.581757612526, acc: 0.759999968112
Epoch: 1880;  D loss: 0.457499981858, acc: 0.725718740374; G loss: 0.643384590745, acc: 0.62999998033
Epoch: 1890;  D loss: 0.399999985471, acc: 0.749962698668; G loss: 0.690765462816, acc: 0.549999963492
Epoch: 1900;  D loss: 0.439999985509, acc: 0.722154457122; G loss: 0.689857393503, acc: 0.504999993369
Epoch: 1910;  D loss: 0.509999986738, acc: 0.702236138284; G loss: 0.686714507639, acc: 0.554999984801
Epoch: 1920;  D loss: 0.497499989346, acc: 0.709303632379; G loss: 0.657699607313, acc: 0.614999968559
Epoch: 1930;  D loss: 0.527499983087, acc: 0.698317717761; G loss: 0.716610893607, acc: 0.494999974966
Epoch: 1940;  D loss: 0.514999995008, acc: 0.7050495781; G loss: 0.696193173528, acc: 0.529999982566
Epoch: 1950;  D loss: 0.579999981448, acc: 0.671861022711; G loss: 0.637012369931, acc: 0.609999984503
Epoch: 1960;  D loss: 0.612499987707, acc: 0.651157073677; G loss: 0.605833172798, acc: 0.639999974519
Epoch: 1970;  D loss: 0.624999988824, acc: 0.642315234989; G loss: 0.580922804773, acc: 0.669999964535
Epoch: 1980;  D loss: 0.579999973997, acc: 0.666327372193; G loss: 0.568302437663, acc: 0.739999979734
Epoch: 1990;  D loss: 0.53499998711, acc: 0.687356200069; G loss: 0.62870170176, acc: 0.62999997288
Epoch: 2000;  D loss: 0.547499977052, acc: 0.683990459889; G loss: 0.618958227336, acc: 0.664999976754
Epoch: 2010;  D loss: 0.65499997139, acc: 0.626045387238; G loss: 0.542234040797, acc: 0.774999968708
Epoch: 2020;  D loss: 0.569999990985, acc: 0.66960484162; G loss: 0.561645150185, acc: 0.769999973476
Epoch: 2030;  D loss: 0.497499980032, acc: 0.694854572415; G loss: 0.598846256733, acc: 0.634999994189
Epoch: 2040;  D loss: 0.487499987707, acc: 0.712404534221; G loss: 0.582330971956, acc: 0.739999972284
Epoch: 2050;  D loss: 0.632499976084, acc: 0.65616999194; G loss: 0.639209710062, acc: 0.624999970198
Epoch: 2060;  D loss: 0.667499979958, acc: 0.625575989485; G loss: 0.574082240462, acc: 0.674999982119
Epoch: 2070;  D loss: 0.587499978021, acc: 0.654372723773; G loss: 0.80757471174, acc: 0.469999983907
Epoch: 2080;  D loss: 0.522499982268, acc: 0.717043016106; G loss: 0.80567804724, acc: 0.299999993294
Epoch: 2090;  D loss: 0.559999976307, acc: 0.681495185941; G loss: 0.734629802406, acc: 0.444999985397
Epoch: 2100;  D loss: 0.604999972507, acc: 0.656228873879; G loss: 0.705448366702, acc: 0.509999986738
Epoch: 2110;  D loss: 0.17249999335, acc: 1.06195688993; G loss: 0.657697036862, acc: 0.614999979734
Epoch: 2120;  D loss: 0.599999977276, acc: 0.672184962779; G loss: 0.862356103957, acc: 0.23999999091
Epoch: 2130;  D loss: 0.582499969751, acc: 0.671922042966; G loss: 0.7351532951, acc: 0.469999998808
Epoch: 2140;  D loss: 0.334999988787, acc: 0.786284547299; G loss: 0.664167396724, acc: 0.594999980181
Epoch: 2150;  D loss: 0.419999986887, acc: 0.737470313907; G loss: 0.699031732976, acc: 0.52999997884
Epoch: 2160;  D loss: 0.44749998115, acc: 0.734742518514; G loss: 0.735898464918, acc: 0.399999979883
Epoch: 2170;  D loss: 0.409999985248, acc: 0.748971402645; G loss: 0.817692749202, acc: 0.299999989569
Epoch: 2180;  D loss: 0.559999983758, acc: 0.694605927914; G loss: 0.900171265006, acc: 0.154999994673
Epoch: 2190;  D loss: 0.404999990948, acc: 0.751290529966; G loss: 0.701731331646, acc: 0.514999978244
Epoch: 2200;  D loss: 0.507499992847, acc: 0.70194234699; G loss: 0.733297072351, acc: 0.424999982119
Epoch: 2210;  D loss: 0.572499969974, acc: 0.685468714684; G loss: 0.761935420334, acc: 0.354999985546
Epoch: 2220;  D loss: 0.327499995008, acc: 0.810287952423; G loss: 0.987715020776, acc: 0.219999985769
Epoch: 2230;  D loss: 0.674999974668, acc: 0.598141416907; G loss: 0.733662717044, acc: 0.409999981523
Epoch: 2240;  D loss: 0.317499991041, acc: 0.806693181396; G loss: 0.658601559699, acc: 0.619999974966
Epoch: 2250;  D loss: 0.474999986589, acc: 0.708029393107; G loss: 0.703250326216, acc: 0.484999977052
Epoch: 2260;  D loss: 0.594999980181, acc: 0.66089021042; G loss: 0.778987444937, acc: 0.314999990165
Epoch: 2270;  D loss: 0.357499987818, acc: 0.786254379898; G loss: 0.846284933388, acc: 0.314999984577
Epoch: 2280;  D loss: 0.619999980554, acc: 0.653579156846; G loss: 0.781585641205, acc: 0.289999991655
Epoch: 2290;  D loss: 0.46749997884, acc: 0.718725364655; G loss: 0.67127776891, acc: 0.559999980032
Epoch: 2300;  D loss: 0.439999983646, acc: 0.723551038653; G loss: 0.706114940345, acc: 0.514999985695
Epoch: 2310;  D loss: 0.559999989346, acc: 0.685790482908; G loss: 0.759966552258, acc: 0.339999993332
Epoch: 2320;  D loss: 0.409999991767, acc: 0.740385700017; G loss: 0.737994126976, acc: 0.414999984205
Epoch: 2330;  D loss: 0.39499999024, acc: 0.751509979367; G loss: 0.793389372528, acc: 0.314999988303
Epoch: 2340;  D loss: 0.574999984354, acc: 0.669627912343; G loss: 0.738343790174, acc: 0.409999974072
Epoch: 2350;  D loss: 0.369999988005, acc: 0.744001947343; G loss: 0.675119869411, acc: 0.544999990612
Epoch: 2360;  D loss: 0.564999982715, acc: 0.67858960852; G loss: 0.7568667382, acc: 0.42999997735
Epoch: 2370;  D loss: 0.502499980852, acc: 0.707224804908; G loss: 0.758270472288, acc: 0.394999980927
Epoch: 2380;  D loss: 0.594999978319, acc: 0.683819659054; G loss: 0.739005848765, acc: 0.434999987483
Epoch: 2390;  D loss: 0.499999986961, acc: 0.704059842974; G loss: 0.683723330498, acc: 0.584999997169
Epoch: 2400;  D loss: 0.519999982789, acc: 0.691894069314; G loss: 0.728889279068, acc: 0.379999984056
Epoch: 2410;  D loss: 0.532499985769, acc: 0.692528858781; G loss: 0.731958113611, acc: 0.429999988526
Epoch: 2420;  D loss: 0.40249998495, acc: 0.735470458865; G loss: 0.790716126561, acc: 0.364999985322
Epoch: 2430;  D loss: 0.512499978766, acc: 0.704484399408; G loss: 0.847798056901, acc: 0.214999997988
Epoch: 2440;  D loss: 0.594999983907, acc: 0.599127981812; G loss: 0.684315130115, acc: 0.57999996841
Epoch: 2450;  D loss: 0.507499974221, acc: 0.733868952841; G loss: 0.795791000128, acc: 0.219999991357
Epoch: 2460;  D loss: 0.947499979287, acc: 0.441303513944; G loss: 0.85405946523, acc: 0.0599999986589
Epoch: 2470;  D loss: 0.804999973625, acc: 0.59074383229; G loss: 0.821013316512, acc: 0.189999995753
Epoch: 2480;  D loss: 0.507499981672, acc: 0.70298756659; G loss: 0.824264608324, acc: 0.239999995567
Epoch: 2490;  D loss: 0.55999997817, acc: 0.687372252345; G loss: 0.83897767216, acc: 0.2149999924
Epoch: 2500;  D loss: 0.587499989197, acc: 0.686858534813; G loss: 0.734243094921, acc: 0.409999985248
Epoch: 2510;  D loss: 0.54749998264, acc: 0.683829277754; G loss: 0.741548314691, acc: 0.379999984056
Epoch: 2520;  D loss: 0.517499981448, acc: 0.704435374588; G loss: 0.742416530848, acc: 0.379999984056
Epoch: 2530;  D loss: 0.502499982715, acc: 0.706722438335; G loss: 0.710065692663, acc: 0.454999979585
Epoch: 2540;  D loss: 0.707499980927, acc: 0.592189278454; G loss: 0.777616977692, acc: 0.334999989718
Epoch: 2550;  D loss: 0.439999987371, acc: 0.730750516057; G loss: 0.599185168743, acc: 0.724999979138
Epoch: 2560;  D loss: 0.419999992475, acc: 0.73451924324; G loss: 0.751666419208, acc: 0.359999984503
Epoch: 2570;  D loss: 0.424999984913, acc: 0.734795566648; G loss: 0.739873938262, acc: 0.399999987334
Epoch: 2580;  D loss: 0.599999990314, acc: 0.665056280792; G loss: 0.80531090498, acc: 0.299999993294
Epoch: 2590;  D loss: 0.507499985397, acc: 0.694839350879; G loss: 0.706692874432, acc: 0.469999995083
Epoch: 2600;  D loss: 0.467499982566, acc: 0.714140359312; G loss: 0.738450363278, acc: 0.389999987558
Epoch: 2610;  D loss: 0.444999983534, acc: 0.718466728926; G loss: 0.738831743598, acc: 0.389999983832
Epoch: 2620;  D loss: 0.442499989644, acc: 0.720481555909; G loss: 0.744926124811, acc: 0.369999988005
Epoch: 2630;  D loss: 0.487499983981, acc: 0.707932174206; G loss: 0.725456878543, acc: 0.419999992475
Epoch: 2640;  D loss: 0.539999989793, acc: 0.687954068184; G loss: 0.727771155536, acc: 0.424999982119
Epoch: 2650;  D loss: 0.562499975786, acc: 0.682425383478; G loss: 0.751452058554, acc: 0.359999984503
Epoch: 2660;  D loss: 0.577499974519, acc: 0.676538147032; G loss: 0.730851404369, acc: 0.439999981783
Epoch: 2670;  D loss: 0.754999969155, acc: 0.583357553929; G loss: 0.788542039692, acc: 0.294999987818
Epoch: 2680;  D loss: 0.437499985099, acc: 0.746782321483; G loss: 0.713545888662, acc: 0.454999987036
Epoch: 2690;  D loss: 0.457499989308, acc: 0.70933342725; G loss: 0.694760560989, acc: 0.524999983609
Epoch: 2700;  D loss: 0.667499978095, acc: 0.641618683934; G loss: 0.790296398103, acc: 0.254999987781
Epoch: 2710;  D loss: 0.69249997288, acc: 0.631230913103; G loss: 0.821604467928, acc: 0.249999991618
Epoch: 2720;  D loss: 0.557499974966, acc: 0.689108211547; G loss: 0.816102489829, acc: 0.279999994673
Epoch: 2730;  D loss: 0.53999998793, acc: 0.697215955704; G loss: 0.707573331892, acc: 0.51499998197
Epoch: 2740;  D loss: 0.422499984503, acc: 0.737513851374; G loss: 0.715543575585, acc: 0.43999998644
Epoch: 2750;  D loss: 0.544999977574, acc: 0.683386914432; G loss: 0.73319683224, acc: 0.38499998115
Epoch: 2760;  D loss: 0.442499982193, acc: 0.712825279683; G loss: 0.716515205801, acc: 0.479999989271
Epoch: 2770;  D loss: 0.469999989495, acc: 0.714585509151; G loss: 0.766815021634, acc: 0.314999993891
Epoch: 2780;  D loss: 0.634999984875, acc: 0.660682875663; G loss: 0.729217901826, acc: 0.429999995977
Epoch: 2790;  D loss: 0.512499988079, acc: 0.693668723106; G loss: 0.734339654446, acc: 0.429999984801
Epoch: 2800;  D loss: 0.564999978989, acc: 0.685174886137; G loss: 0.708942197263, acc: 0.474999979138
Epoch: 2810;  D loss: 0.517499988899, acc: 0.703513897955; G loss: 0.736470043659, acc: 0.434999983758
Epoch: 2820;  D loss: 0.552499979734, acc: 0.674697726965; G loss: 0.669835686684, acc: 0.589999988675
Epoch: 2830;  D loss: 0.449999986216, acc: 0.715912640095; G loss: 0.682357333601, acc: 0.579999987036
Epoch: 2840;  D loss: 0.544999977574, acc: 0.694284632802; G loss: 0.725082419813, acc: 0.444999977946
Epoch: 2850;  D loss: 0.542499981821, acc: 0.679033175111; G loss: 0.752337403595, acc: 0.334999989718
Epoch: 2860;  D loss: 0.539999982342, acc: 0.688438605517; G loss: 0.766921959817, acc: 0.329999991693
Epoch: 2870;  D loss: 0.559999976307, acc: 0.67440078035; G loss: 0.665578685701, acc: 0.574999988079
Epoch: 2880;  D loss: 0.517499983311, acc: 0.694395322353; G loss: 0.663481250405, acc: 0.599999979138
Epoch: 2890;  D loss: 0.589999981225, acc: 0.671100985259; G loss: 0.66248562932, acc: 0.609999977052
Epoch: 2900;  D loss: 0.537499979138, acc: 0.6877200827; G loss: 0.673509500921, acc: 0.589999977499
Epoch: 2910;  D loss: 0.652499988675, acc: 0.637019205838; G loss: 0.629607111216, acc: 0.649999976158
Epoch: 2920;  D loss: 0.752499978989, acc: 0.595823530108; G loss: 0.585003726184, acc: 0.739999979734
Epoch: 2930;  D loss: 0.602499982342, acc: 0.658927313983; G loss: 0.571875572205, acc: 0.749999985099
Epoch: 2940;  D loss: 0.704999983311, acc: 0.62062940374; G loss: 0.61921928823, acc: 0.664999976754
Epoch: 2950;  D loss: 0.812499981374, acc: 0.563003327698; G loss: 0.553506989032, acc: 0.72999997437
Epoch: 2960;  D loss: 0.779999967664, acc: 0.568388197571; G loss: 0.523825258017, acc: 0.789999976754
Epoch: 2970;  D loss: 0.79249997437, acc: 0.552632056177; G loss: 0.491191890091, acc: 0.75499998033
Epoch: 2980;  D loss: 0.802499964833, acc: 0.549099110067; G loss: 0.465293694288, acc: 0.82999997586
Epoch: 2990;  D loss: 0.847499977797, acc: 0.495967371389; G loss: 0.432723335922, acc: 0.789999976754
Epoch: 3000;  D loss: 0.812499973923, acc: 0.507148670033; G loss: 0.431953575462, acc: 0.819999970496
Epoch: 3010;  D loss: 0.669999968261, acc: 0.616244431585; G loss: 0.363871727139, acc: 0.934999972582
Epoch: 3020;  D loss: 0.629999976605, acc: 0.626729436219; G loss: 0.484897639602, acc: 0.889999978244
Epoch: 3030;  D loss: 0.749999973923, acc: 0.583311382681; G loss: 0.497289665043, acc: 0.844999969006
Epoch: 3040;  D loss: 0.702499976382, acc: 0.586859121919; G loss: 0.508330710232, acc: 0.809999965131
Epoch: 3050;  D loss: 0.707499977201, acc: 0.626051776111; G loss: 0.512388758361, acc: 0.869999982417
Epoch: 3060;  D loss: 0.699999973178, acc: 0.58864961192; G loss: 0.609485983849, acc: 0.709999985993
Epoch: 3070;  D loss: 0.732499983162, acc: 0.569674240425; G loss: 0.573342248797, acc: 0.759999968112
Epoch: 3080;  D loss: 0.774999972433, acc: 0.556008011103; G loss: 0.510877408087, acc: 0.789999976754
Epoch: 3090;  D loss: 0.857499971986, acc: 0.494253801182; G loss: 0.469989258796, acc: 0.814999975264
Epoch: 3100;  D loss: 0.892499972135, acc: 0.457276521251; G loss: 0.431570969522, acc: 0.819999977946
Epoch: 3110;  D loss: 0.894999973476, acc: 0.418461419642; G loss: 0.331898629665, acc: 0.87999997288
Epoch: 3120;  D loss: 0.899999983609, acc: 0.423026109114; G loss: 0.354582745582, acc: 0.869999974966
Epoch: 3130;  D loss: 0.849999975413, acc: 0.434211010113; G loss: 0.25697985664, acc: 0.924999974668
Epoch: 3140;  D loss: 0.899999976158, acc: 0.391516199335; G loss: 0.270850701258, acc: 0.909999974072
Epoch: 3150;  D loss: 0.784999981523, acc: 0.461475996301; G loss: 0.24063824676, acc: 0.90499997139
Epoch: 3160;  D loss: 0.834999978542, acc: 0.443674068898; G loss: 0.25458545424, acc: 0.919999971986
Epoch: 3170;  D loss: 0.842499975115, acc: 0.447348861024; G loss: 0.226252157241, acc: 0.959999978542
Epoch: 3180;  D loss: 0.824999976903, acc: 0.432962108403; G loss: 0.237195009366, acc: 0.914999969304
Epoch: 3190;  D loss: 0.824999973178, acc: 0.449886694551; G loss: 0.20781589672, acc: 0.924999974668
Epoch: 3200;  D loss: 0.692499974743, acc: 0.570062341169; G loss: 0.207068441436, acc: 0.914999976754
Epoch: 3210;  D loss: 0.704999985173, acc: 0.576326403767; G loss: 0.261002946645, acc: 0.90499997884
Epoch: 3220;  D loss: 0.729999978095, acc: 0.513134902343; G loss: 0.254260793328, acc: 0.914999984205
Epoch: 3230;  D loss: 0.764999970794, acc: 0.470221389085; G loss: 0.302164036781, acc: 0.894999980927
Epoch: 3240;  D loss: 0.722499966621, acc: 0.516517760232; G loss: 0.289932634681, acc: 0.884999975562
Epoch: 3250;  D loss: 0.707499979064, acc: 0.488476760685; G loss: 0.132465076633, acc: 0.90499997139
Epoch: 3260;  D loss: 0.754999969155, acc: 0.510659296066; G loss: 0.201307151467, acc: 0.85499997437
Epoch: 3270;  D loss: 0.814999971539, acc: 0.361458431929; G loss: 0.226049648598, acc: 0.859999969602
Epoch: 3280;  D loss: 0.799999974668, acc: 0.297324581072; G loss: 0.240794504061, acc: 0.749999962747
Epoch: 3290;  D loss: 0.8049999699, acc: 0.37301700376; G loss: 0.194403992966, acc: 0.794999971986
Epoch: 3300;  D loss: 0.784999970347, acc: 0.42719119601; G loss: 0.120707419701, acc: 0.80499997735
Epoch: 3310;  D loss: 0.769999969751, acc: 0.391046861187; G loss: 0.124068532139, acc: 0.814999982715
Epoch: 3320;  D loss: 0.777499970049, acc: 0.326373740099; G loss: 0.141111456789, acc: 0.774999972433
Epoch: 3330;  D loss: 0.787499971688, acc: 0.304530731402; G loss: 0.108310064767, acc: 0.804999984801
Epoch: 3340;  D loss: 0.777499981225, acc: 0.356981101446; G loss: 0.161852432415, acc: 0.769999966025
Epoch: 3350;  D loss: 0.774999983609, acc: 0.279619939625; G loss: 0.13978457544, acc: 0.749999985099
Epoch: 3360;  D loss: 0.79249997437, acc: 0.205216025002; G loss: 0.132378028706, acc: 0.72999997437
Epoch: 3370;  D loss: 0.822499971837, acc: 0.220283407718; G loss: 0.0610065173823, acc: 0.709999978542
Epoch: 3380;  D loss: 0.774999976158, acc: 0.203078903258; G loss: 0.134117209353, acc: 0.794999986887
Epoch: 3390;  D loss: 0.787499975413, acc: 0.157755765133; G loss: 0.128269614652, acc: 0.72999997437
Epoch: 3400;  D loss: 0.747499980032, acc: 0.181052537169; G loss: 0.101632887032, acc: 0.65499997884
Epoch: 3410;  D loss: 0.7424999699, acc: 0.172215994447; G loss: 0.208216513507, acc: 0.689999967813
Epoch: 3420;  D loss: 0.717499975115, acc: 0.1938425228; G loss: 0.145657900721, acc: 0.614999987185
Epoch: 3430;  D loss: 0.75499997288, acc: 0.188883655705; G loss: 0.17128015589, acc: 0.664999984205
Epoch: 3440;  D loss: 0.744999982417, acc: 0.188864673954; G loss: 0.230570830405, acc: 0.684999980032
Epoch: 3450;  D loss: 0.77999997139, acc: 0.205540015362; G loss: 0.157329995185, acc: 0.744999960065
Epoch: 3460;  D loss: 0.764999974519, acc: 0.261974044144; G loss: 0.404774893075, acc: 0.689999975264
Epoch: 3470;  D loss: 0.774999972433, acc: 0.29631748423; G loss: 0.23229296878, acc: 0.764999970794
Epoch: 3480;  D loss: 0.657499967143, acc: 0.504556227475; G loss: 0.30217900686, acc: 0.769999966025
Epoch: 3490;  D loss: 0.67749998346, acc: 0.408809425309; G loss: 0.292060595006, acc: 0.769999973476
Epoch: 3500;  D loss: 0.717499982566, acc: 0.435681968927; G loss: 0.269618954509, acc: 0.759999975562
Epoch: 3510;  D loss: 0.689999971539, acc: 0.427403064445; G loss: 0.206430478022, acc: 0.799999982119
Epoch: 3520;  D loss: 0.782499969006, acc: 0.348117536865; G loss: 0.302177187055, acc: 0.789999961853
Epoch: 3530;  D loss: 0.782499972731, acc: 0.304238019511; G loss: 0.267773093656, acc: 0.794999964535
Epoch: 3540;  D loss: 0.789999980479, acc: 0.310669469181; G loss: 0.252116616815, acc: 0.814999967813
Epoch: 3550;  D loss: 0.794999979436, acc: 0.293192512356; G loss: 0.258856652305, acc: 0.784999966621
Epoch: 3560;  D loss: 0.712499976158, acc: 0.311805153266; G loss: 0.154339127243, acc: 0.77999997884
Epoch: 3570;  D loss: 0.749999977648, acc: 0.228484426159; G loss: 0.198725106195, acc: 0.709999978542
Epoch: 3580;  D loss: 0.742499973625, acc: 0.2310948167; G loss: 0.247710268945, acc: 0.764999978244
Epoch: 3590;  D loss: 0.734999969602, acc: 0.199896344449; G loss: 0.233259420842, acc: 0.724999979138
Epoch: 3600;  D loss: 0.719999972731, acc: 0.22463470418; G loss: 0.24772551097, acc: 0.699999965727
Epoch: 3610;  D loss: 0.69499996677, acc: 0.30683171656; G loss: 0.171495646238, acc: 0.664999976754
Epoch: 3620;  D loss: 0.64249997586, acc: 0.278378326446; G loss: 0.15389662236, acc: 0.594999991357
Epoch: 3630;  D loss: 0.712499968708, acc: 0.242710545193; G loss: 0.194705242291, acc: 0.574999976903
Epoch: 3640;  D loss: 0.712499976158, acc: 0.274531330913; G loss: 0.263341634534, acc: 0.719999976456
Epoch: 3650;  D loss: 0.422499990091, acc: 2.36505698413; G loss: 0.414099670947, acc: 0.739999987185
Epoch: 3660;  D loss: 0.194999994244, acc: 3.69954152405; G loss: 0.0906636961736, acc: 0.939999975264
Epoch: 3670;  D loss: 0.197499992792, acc: 2.65969462693; G loss: 0.146592353471, acc: 0.944999977946
Epoch: 3680;  D loss: 0.179999992717, acc: 2.79771085829; G loss: 0.179879881442, acc: 0.964999981225
Epoch: 3690;  D loss: 0.192499992903, acc: 2.21819409728; G loss: 0.142722601071, acc: 0.964999981225
Epoch: 3700;  D loss: 0.224999993108, acc: 2.06073444709; G loss: 0.14643085096, acc: 0.964999988675
Epoch: 3710;  D loss: 0.169999994338, acc: 2.11185726523; G loss: 0.172866391018, acc: 0.959999978542
Epoch: 3720;  D loss: 0.237499990966, acc: 1.78581746295; G loss: 0.247848331928, acc: 0.919999979436
Epoch: 3730;  D loss: 0.212499990128, acc: 1.93825186044; G loss: 0.165014207363, acc: 0.974999986589
Epoch: 3740;  D loss: 0.202499994542, acc: 1.97895068675; G loss: 0.2215404585, acc: 0.949999980628
Epoch: 3750;  D loss: 0.174999992363, acc: 2.02866712958; G loss: 0.220341188833, acc: 0.919999971986
Epoch: 3760;  D loss: 0.222499991301, acc: 1.8886198625; G loss: 0.284055644646, acc: 0.909999981523
Epoch: 3770;  D loss: 0.204999991227, acc: 1.84242699295; G loss: 0.268362289295, acc: 0.934999980032
Epoch: 3780;  D loss: 0.232499989681, acc: 1.79309967533; G loss: 0.331510502845, acc: 0.864999979734
Epoch: 3790;  D loss: 0.129999996163, acc: 3.05477391928; G loss: 0.154758374207, acc: 0.969999983907
Epoch: 3800;  D loss: 0.127499996219, acc: 2.74750786275; G loss: 0.249343313277, acc: 0.944999977946
Epoch: 3810;  D loss: 0.212499990594, acc: 2.06098291278; G loss: 0.363307338208, acc: 0.839999966323
Epoch: 3820;  D loss: 0.17749999417, acc: 2.67070028186; G loss: 0.198073327541, acc: 0.919999994338
Epoch: 3830;  D loss: 0.217499993276, acc: 1.83987760544; G loss: 0.32277530618, acc: 0.864999979734
Epoch: 3840;  D loss: 0.152499994729, acc: 2.51930592954; G loss: 0.40242420882, acc: 0.869999982417
Epoch: 3850;  D loss: 0.194999996573, acc: 1.88591461629; G loss: 0.384538553655, acc: 0.824999980628
Epoch: 3860;  D loss: 0.164999995381, acc: 2.2545511052; G loss: 0.346932433546, acc: 0.854999981821
Epoch: 3870;  D loss: 0.187499993481, acc: 1.99501187354; G loss: 0.355294253677, acc: 0.874999962747
Epoch: 3880;  D loss: 0.164999996312, acc: 1.66978307068; G loss: 0.397342927754, acc: 0.849999964237
Epoch: 3890;  D loss: 0.202499991748, acc: 1.55795945227; G loss: 0.454032316804, acc: 0.82999997586
Epoch: 3900;  D loss: 0.194999994244, acc: 1.5051548481; G loss: 0.405075505376, acc: 0.82999997586
Epoch: 3910;  D loss: 0.252499992028, acc: 1.33120808378; G loss: 0.353192582726, acc: 0.884999975562
Epoch: 3920;  D loss: 0.207499993499, acc: 1.45001947135; G loss: 0.379284407943, acc: 0.899999976158
Epoch: 3930;  D loss: 0.22749999119, acc: 1.30167229101; G loss: 0.351726371795, acc: 0.884999983013
Epoch: 3940;  D loss: 0.259999989532, acc: 1.30004538596; G loss: 0.46275177598, acc: 0.789999984205
Epoch: 3950;  D loss: 0.272499991581, acc: 1.18735653907; G loss: 0.344136089087, acc: 0.909999981523
Epoch: 3960;  D loss: 0.254999990575, acc: 1.13532736525; G loss: 0.382798682898, acc: 0.854999981821
Epoch: 3970;  D loss: 0.232499994803, acc: 1.17647890747; G loss: 0.391652125865, acc: 0.884999975562
Epoch: 3980;  D loss: 0.267499991693, acc: 1.06223388761; G loss: 0.265307309106, acc: 0.939999975264
Epoch: 3990;  D loss: 0.232499988284, acc: 1.21868058294; G loss: 0.241307249293, acc: 0.969999983907
Epoch: 4000;  D loss: 0.257499989122, acc: 1.10529217497; G loss: 0.332031153142, acc: 0.909999981523
Epoch: 4010;  D loss: 0.214999992866, acc: 1.19544525817; G loss: 0.35645538941, acc: 0.889999978244
Epoch: 4020;  D loss: 0.274999991991, acc: 1.18238898367; G loss: 0.334859315306, acc: 0.919999979436
Epoch: 4030;  D loss: 0.299999989569, acc: 0.994899019599; G loss: 0.364404723048, acc: 0.884999975562
Epoch: 4040;  D loss: 0.292499991134, acc: 1.01536365598; G loss: 0.369790948927, acc: 0.869999982417
Epoch: 4050;  D loss: 0.28499998711, acc: 0.980215784162; G loss: 0.42144966498, acc: 0.839999966323
Epoch: 4060;  D loss: 0.287499991711, acc: 1.0422857292; G loss: 0.364486206323, acc: 0.894999973476
Epoch: 4070;  D loss: 0.27999999281, acc: 1.00927304849; G loss: 0.34574971348, acc: 0.944999977946
Epoch: 4080;  D loss: 0.307499988936, acc: 1.07272697613; G loss: 0.396192707121, acc: 0.874999977648
Epoch: 4090;  D loss: 0.354999985546, acc: 0.92584624514; G loss: 0.385314818472, acc: 0.87999998033
Epoch: 4100;  D loss: 0.362499990966, acc: 0.920740809292; G loss: 0.413887675852, acc: 0.894999980927
Epoch: 4110;  D loss: 0.312499987893, acc: 0.919789589942; G loss: 0.364624552429, acc: 0.899999976158
Epoch: 4120;  D loss: 0.349999982864, acc: 0.940385535359; G loss: 0.395330060273, acc: 0.894999980927
Epoch: 4130;  D loss: 0.402499991469, acc: 0.849769033492; G loss: 0.358694467694, acc: 0.909999974072
Epoch: 4140;  D loss: 0.324999990873, acc: 0.880969818681; G loss: 0.386477760971, acc: 0.90499997884
Epoch: 4150;  D loss: 0.332499991171, acc: 0.887168966234; G loss: 0.365798361599, acc: 0.939999982715
Epoch: 4160;  D loss: 0.394999988377, acc: 0.84194323048; G loss: 0.399330370128, acc: 0.90499997139
Epoch: 4170;  D loss: 0.374999993481, acc: 0.856362104416; G loss: 0.403213262558, acc: 0.904999986291
Epoch: 4180;  D loss: 0.442499977536, acc: 0.798981323838; G loss: 0.360117029399, acc: 0.95499997586
Epoch: 4190;  D loss: 0.447499986738, acc: 0.763979006559; G loss: 0.417040377855, acc: 0.884999975562
Epoch: 4200;  D loss: 0.48499998264, acc: 0.745093114674; G loss: 0.405426263809, acc: 0.889999978244
Epoch: 4210;  D loss: 0.534999985248, acc: 0.710760846734; G loss: 0.426265470684, acc: 0.864999972284
Epoch: 4220;  D loss: 0.489999985322, acc: 0.735244639218; G loss: 0.423571556807, acc: 0.889999963343
Epoch: 4230;  D loss: 0.519999984652, acc: 0.751006733626; G loss: 0.422472372651, acc: 0.874999970198
Epoch: 4240;  D loss: 0.567499982193, acc: 0.663823310286; G loss: 0.42087835446, acc: 0.919999971986
Epoch: 4250;  D loss: 0.534999972209, acc: 0.68411007151; G loss: 0.385945882648, acc: 0.924999974668
Epoch: 4260;  D loss: 0.599999986589, acc: 0.678430184722; G loss: 0.40357484296, acc: 0.909999966621
Epoch: 4270;  D loss: 0.647499967366, acc: 0.618825413287; G loss: 0.413525350392, acc: 0.889999978244
Epoch: 4280;  D loss: 0.607499975711, acc: 0.64937980473; G loss: 0.348848741502, acc: 0.914999976754
Epoch: 4290;  D loss: 0.537499982864, acc: 0.75499529019; G loss: 0.414674285799, acc: 0.839999973774
Epoch: 4300;  D loss: 0.392499988899, acc: 0.794695310295; G loss: 0.385869320482, acc: 0.874999985099
Epoch: 4310;  D loss: 0.25749999471, acc: 8.32547339797; G loss: 1.19209332183e-07, acc: 0.0299999993294
Epoch: 4320;  D loss: 0.45749999024, acc: 8.14114743471; G loss: 1.19209332183e-07, acc: 0.00499999988824
Epoch: 4330;  D loss: 0.472499981523, acc: 8.07805997133; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4340;  D loss: 0.489999981597, acc: 8.02369090915; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4350;  D loss: 0.489999981597, acc: 7.99480912089; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4360;  D loss: 0.472499981523, acc: 7.98957657814; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4370;  D loss: 0.459999986924, acc: 7.98320439458; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4380;  D loss: 0.459999991581, acc: 7.97926512361; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4390;  D loss: 0.452499978244, acc: 7.97741086781; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4400;  D loss: 0.432499988005, acc: 7.97504287958; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4410;  D loss: 0.409999986179, acc: 7.97434064746; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4420;  D loss: 0.429999981076, acc: 7.97425422072; G loss: 1.19209332183e-07, acc: 0.0
Epoch: 4430;  D loss: 0.397499985993, acc: 7.9728692174; G loss: 1.19209332183e-07, acc: 0.0




run10:

  working with the new bigger dataset

  i think adam is not the way to go.

  #################################################################################
  # Globals
  #################################################################################

  RUN = 10

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( Adam(lr=0.000002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( Adam(lr=0.00008, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( Adam(lr=0.00008, clipvalue=1.0, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )



run9:

  tjust trying shit

run8:

  #################################################################################
  # Globals
  #################################################################################

  RUN = 8

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( Adam(lr=0.000002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( Adam(lr=0.00008, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( Adam(lr=0.00008, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )


run7:

  trying adam


  #################################################################################
  # Globals
  #################################################################################

  RUN = 7

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( Adam(lr=0.000002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( Adam(lr=0.00008, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( Adam(lr=0.000001, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )




run6:


  #################################################################################
  # Globals
  #################################################################################

  RUN = 6

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( RMSprop(lr=0.00002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( RMSprop(lr=0.08, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( Adam(lr=0.0001, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )



run5:

  collapsed after like 2k epochs

  #################################################################################
  # Globals
  #################################################################################

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( RMSprop(lr=0.00002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( RMSprop(lr=0.08, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( RMSprop(lr=0.00001, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )






run4:

  we got stuck somewhere. i belive we need to focus on the early learning or we are getting stuck in a local minima

  #################################################################################
  # Globals
  #################################################################################

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 15000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( RMSprop(lr=0.00002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( RMSprop(lr=0.08, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( RMSprop(lr=0.001, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )

  Discrimnator:

  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  conv2d_1 (Conv2D)            (None, 53, 33, 32)        896
  _________________________________________________________________
  activation_1 (Activation)    (None, 53, 33, 32)        0
  _________________________________________________________________
  conv2d_2 (Conv2D)            (None, 51, 31, 32)        9248
  _________________________________________________________________
  activation_2 (Activation)    (None, 51, 31, 32)        0
  _________________________________________________________________
  max_pooling2d_1 (MaxPooling2 (None, 25, 15, 32)        0
  _________________________________________________________________
  dropout_1 (Dropout)          (None, 25, 15, 32)        0
  _________________________________________________________________
  conv2d_3 (Conv2D)            (None, 25, 15, 64)        18496
  _________________________________________________________________
  activation_3 (Activation)    (None, 25, 15, 64)        0
  _________________________________________________________________
  conv2d_4 (Conv2D)            (None, 23, 13, 64)        36928
  _________________________________________________________________
  activation_4 (Activation)    (None, 23, 13, 64)        0
  _________________________________________________________________
  max_pooling2d_2 (MaxPooling2 (None, 11, 6, 64)         0
  _________________________________________________________________
  dropout_2 (Dropout)          (None, 11, 6, 64)         0
  _________________________________________________________________
  flatten_1 (Flatten)          (None, 4224)              0
  _________________________________________________________________
  dense_1 (Dense)              (None, 512)               2163200
  _________________________________________________________________
  activation_5 (Activation)    (None, 512)               0
  _________________________________________________________________
  dropout_3 (Dropout)          (None, 512)               0
  _________________________________________________________________
  dense_2 (Dense)              (None, 1)                 513
  _________________________________________________________________
  activation_6 (Activation)    (None, 1)                 0
  =================================================================
  Total params: 2,229,281
  Trainable params: 2,229,281
  Non-trainable params: 0
  _________________________________________________________________
  Generator:

  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  dense_3 (Dense)              (None, 100)               10100
  _________________________________________________________________
  activation_7 (Activation)    (None, 100)               0
  _________________________________________________________________
  dense_4 (Dense)              (None, 300)               30300
  _________________________________________________________________
  activation_8 (Activation)    (None, 300)               0
  _________________________________________________________________
  dense_5 (Dense)              (None, 312)               93912
  _________________________________________________________________
  dropout_4 (Dropout)          (None, 312)               0
  _________________________________________________________________
  reshape_1 (Reshape)          (None, 13, 8, 3)          0
  _________________________________________________________________
  conv2d_transpose_1 (Conv2DTr (None, 26, 16, 64)        1792
  _________________________________________________________________
  activation_9 (Activation)    (None, 26, 16, 64)        0
  _________________________________________________________________
  conv2d_transpose_2 (Conv2DTr (None, 53, 33, 64)        36928
  _________________________________________________________________
  activation_10 (Activation)   (None, 53, 33, 64)        0
  _________________________________________________________________
  conv2d_transpose_3 (Conv2DTr (None, 53, 33, 64)        36928
  _________________________________________________________________
  activation_11 (Activation)   (None, 53, 33, 64)        0
  _________________________________________________________________
  conv2d_transpose_4 (Conv2DTr (None, 53, 33, 3)         1731
  _________________________________________________________________
  activation_12 (Activation)   (None, 53, 33, 3)         0
  =================================================================
  Total params: 211,691
  Trainable params: 211,691
  Non-trainable params: 0

  Epoch: 9120;  D loss: 0.00249999994412, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9130;  D loss: 0.0, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9140;  D loss: 0.00249999994412, acc: 7.97119253874; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9150;  D loss: 0.0, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9160;  D loss: 0.00249999994412, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9170;  D loss: 0.00249999994412, acc: 7.97119265795; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9180;  D loss: 0.00249999994412, acc: 7.97119253874; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9190;  D loss: 0.0, acc: 7.97119265795; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9200;  D loss: 0.0, acc: 7.97119250894; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9210;  D loss: 0.00249999994412, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9220;  D loss: 0.00249999994412, acc: 7.97119241953; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9230;  D loss: 0.00999999977648, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9240;  D loss: 0.0, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9250;  D loss: 0.0, acc: 7.97119253874; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9260;  D loss: 0.00249999994412, acc: 7.97119253874; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9270;  D loss: 0.0, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9280;  D loss: 0.00499999988824, acc: 7.97119256854; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9290;  D loss: 0.0, acc: 7.97119253874; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9300;  D loss: 0.0, acc: 7.97119238973; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9310;  D loss: 0.0, acc: 7.97119241953; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9320;  D loss: 0.00249999994412, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9330;  D loss: 0.00249999994412, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9340;  D loss: 0.0, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9350;  D loss: 0.00499999988824, acc: 7.97119262815; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9360;  D loss: 0.0, acc: 7.97119250894; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9370;  D loss: 0.00249999994412, acc: 7.97119262815; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9380;  D loss: 0.00499999988824, acc: 7.97119268775; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9390;  D loss: 0.0, acc: 7.97119250894; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9400;  D loss: 0.00499999988824, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9410;  D loss: 0.0, acc: 7.97119262815; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9420;  D loss: 0.0, acc: 7.97119256854; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9430;  D loss: 0.00249999994412, acc: 7.97119241953; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9440;  D loss: 0.0, acc: 7.97119259834; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9450;  D loss: 0.0, acc: 7.97119253874; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9460;  D loss: 0.00499999988824, acc: 7.97119247913; G loss: 1.19209332183e-07, acc: 0.0
  Epoch: 9470;  D loss: 0.00499999988824, acc: 7.97119250894; G loss: 1.19209332183e-07, acc: 0.0




run3:

  hopefully we train D this time. it seems it made things worse


  #################################################################################
  # Globals
  #################################################################################

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 500

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( RMSprop(lr=0.00002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( RMSprop(lr=0.08, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( RMSprop(lr=0.0001, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )


  Discrimnator:

  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  conv2d_1 (Conv2D)            (None, 53, 33, 32)        896
  _________________________________________________________________
  activation_1 (Activation)    (None, 53, 33, 32)        0
  _________________________________________________________________
  conv2d_2 (Conv2D)            (None, 51, 31, 32)        9248
  _________________________________________________________________
  activation_2 (Activation)    (None, 51, 31, 32)        0
  _________________________________________________________________
  max_pooling2d_1 (MaxPooling2 (None, 25, 15, 32)        0
  _________________________________________________________________
  dropout_1 (Dropout)          (None, 25, 15, 32)        0
  _________________________________________________________________
  conv2d_3 (Conv2D)            (None, 25, 15, 64)        18496
  _________________________________________________________________
  activation_3 (Activation)    (None, 25, 15, 64)        0
  _________________________________________________________________
  conv2d_4 (Conv2D)            (None, 23, 13, 64)        36928
  _________________________________________________________________
  activation_4 (Activation)    (None, 23, 13, 64)        0
  _________________________________________________________________
  max_pooling2d_2 (MaxPooling2 (None, 11, 6, 64)         0
  _________________________________________________________________
  dropout_2 (Dropout)          (None, 11, 6, 64)         0
  _________________________________________________________________
  flatten_1 (Flatten)          (None, 4224)              0
  _________________________________________________________________
  dense_1 (Dense)              (None, 512)               2163200
  _________________________________________________________________
  activation_5 (Activation)    (None, 512)               0
  _________________________________________________________________
  dropout_3 (Dropout)          (None, 512)               0
  _________________________________________________________________
  dense_2 (Dense)              (None, 1)                 513
  _________________________________________________________________
  activation_6 (Activation)    (None, 1)                 0
  =================================================================
  Total params: 2,229,281
  Trainable params: 2,229,281
  Non-trainable params: 0
  _________________________________________________________________
  Generator:

  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  dense_3 (Dense)              (None, 100)               10100
  _________________________________________________________________
  activation_7 (Activation)    (None, 100)               0
  _________________________________________________________________
  dense_4 (Dense)              (None, 300)               30300
  _________________________________________________________________
  activation_8 (Activation)    (None, 300)               0
  _________________________________________________________________
  dense_5 (Dense)              (None, 312)               93912
  _________________________________________________________________
  dropout_4 (Dropout)          (None, 312)               0
  _________________________________________________________________
  reshape_1 (Reshape)          (None, 13, 8, 3)          0
  _________________________________________________________________
  conv2d_transpose_1 (Conv2DTr (None, 26, 16, 64)        1792
  _________________________________________________________________
  activation_9 (Activation)    (None, 26, 16, 64)        0
  _________________________________________________________________
  conv2d_transpose_2 (Conv2DTr (None, 53, 33, 64)        36928
  _________________________________________________________________
  activation_10 (Activation)   (None, 53, 33, 64)        0
  _________________________________________________________________
  conv2d_transpose_3 (Conv2DTr (None, 53, 33, 64)        36928
  _________________________________________________________________
  activation_11 (Activation)   (None, 53, 33, 64)        0
  _________________________________________________________________
  conv2d_transpose_4 (Conv2DTr (None, 53, 33, 3)         1731
  _________________________________________________________________
  activation_12 (Activation)   (None, 53, 33, 3)         0
  =================================================================
  Total params: 211,691
  Trainable params: 211,691
  Non-trainable params: 0






run 2:

  seems like we are getting there. need to train longer probably. it also looks like i never trained D

  #number of samples per iteration
  m = 200

  #epochs updating D
  k = 1

  #epochs
  epochs = 5000

  #batch_size
  batch_size = 25

  # optimizers
  d_opt = ( RMSprop(lr=0.00002, decay=6e-8), 'binary_crossentropy' , ['accuracy'] )
  g_opt = ( RMSprop(lr=0.08, clipvalue=1.0, decay=6e-8) , 'binary_crossentropy' , ['accuracy'] )
  stacked_opt = ( RMSprop(lr=0.0001, decay=3e-8), 'binary_crossentropy' , ['accuracy'] )



  Discrimnator:

  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  conv2d_1 (Conv2D)            (None, 53, 33, 32)        896
  _________________________________________________________________
  activation_1 (Activation)    (None, 53, 33, 32)        0
  _________________________________________________________________
  conv2d_2 (Conv2D)            (None, 51, 31, 32)        9248
  _________________________________________________________________
  activation_2 (Activation)    (None, 51, 31, 32)        0
  _________________________________________________________________
  max_pooling2d_1 (MaxPooling2 (None, 25, 15, 32)        0
  _________________________________________________________________
  dropout_1 (Dropout)          (None, 25, 15, 32)        0
  _________________________________________________________________
  conv2d_3 (Conv2D)            (None, 25, 15, 64)        18496
  _________________________________________________________________
  activation_3 (Activation)    (None, 25, 15, 64)        0
  _________________________________________________________________
  conv2d_4 (Conv2D)            (None, 23, 13, 64)        36928
  _________________________________________________________________
  activation_4 (Activation)    (None, 23, 13, 64)        0
  _________________________________________________________________
  max_pooling2d_2 (MaxPooling2 (None, 11, 6, 64)         0
  _________________________________________________________________
  dropout_2 (Dropout)          (None, 11, 6, 64)         0
  _________________________________________________________________
  flatten_1 (Flatten)          (None, 4224)              0
  _________________________________________________________________
  dense_1 (Dense)              (None, 512)               2163200
  _________________________________________________________________
  activation_5 (Activation)    (None, 512)               0
  _________________________________________________________________
  dropout_3 (Dropout)          (None, 512)               0
  _________________________________________________________________
  dense_2 (Dense)              (None, 1)                 513
  _________________________________________________________________
  activation_6 (Activation)    (None, 1)                 0
  =================================================================
  Total params: 2,229,281
  Trainable params: 2,229,281
  Non-trainable params: 0
  _________________________________________________________________
  Generator:

  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  dense_3 (Dense)              (None, 100)               10100
  _________________________________________________________________
  activation_7 (Activation)    (None, 100)               0
  _________________________________________________________________
  dense_4 (Dense)              (None, 300)               30300
  _________________________________________________________________
  activation_8 (Activation)    (None, 300)               0
  _________________________________________________________________
  dense_5 (Dense)              (None, 312)               93912
  _________________________________________________________________
  dropout_4 (Dropout)          (None, 312)               0
  _________________________________________________________________
  reshape_1 (Reshape)          (None, 13, 8, 3)          0
  _________________________________________________________________
  conv2d_transpose_1 (Conv2DTr (None, 26, 16, 64)        1792
  _________________________________________________________________
  activation_9 (Activation)    (None, 26, 16, 64)        0
  _________________________________________________________________
  conv2d_transpose_2 (Conv2DTr (None, 53, 33, 64)        36928
  _________________________________________________________________
  activation_10 (Activation)   (None, 53, 33, 64)        0
  _________________________________________________________________
  conv2d_transpose_3 (Conv2DTr (None, 53, 33, 64)        36928
  _________________________________________________________________
  activation_11 (Activation)   (None, 53, 33, 64)        0
  _________________________________________________________________
  conv2d_transpose_4 (Conv2DTr (None, 53, 33, 3)         1731
  _________________________________________________________________
  activation_12 (Activation)   (None, 53, 33, 3)         0
  =================================================================
  Total params: 211,691
  Trainable params: 211,691
  Non-trainable params: 0




run 1:

  very low learning rate on D very high learning rate on G. Got some interesting results in between but we seem to forget them all. maybe learning rate too high?

  Data:
  random samples each epoch


  stacked optimizer

  model.compile( loss = 'binary_crossentropy',
                      optimizer = RMSprop(lr=0.0001, decay=3e-8),
          metrics=['accuracy'])

  Generator
      model = Sequential()
      model.add( Dense( gen_inputs , input_shape = (gen_inputs, ) ) )
      model.add( Activation( 'relu' ) )
      model.add( Dense( 300 ) )
      model.add( Activation( 'relu' ) )
      model.add( Dense( 312 ) )
      model.add( Dropout( 0.4 ) )

      model.add( Reshape( ( 13, 8, 3  ) ) )
      model.add( Conv2DTranspose( 64, (3, 3), padding = 'same', strides= ( 2, 2 ) ) )
      model.add( Activation( 'relu' ) )
      model.add( Conv2DTranspose( 64, (3, 3), strides= ( 2, 2 ) ) )
      model.add( Activation( 'relu' ) )

      model.add( Conv2DTranspose( 64, (3, 3), padding = 'same' ) )
      model.add( Activation( 'relu' ) )

      model.add( Conv2DTranspose( 3, (3, 3), padding = 'same' ) )
      model.add( Activation( 'relu' ) )

      optimizer = RMSprop(lr=0.08, clipvalue=1.0, decay=6e-8)

      model.compile( loss='binary_crossentropy',
                      optimizer=optimizer,
                      metrics=['accuracy'] )

  Discrimnator:
      model = Sequential()
      model.add( Conv2D( 32, (3, 3), padding='same', input_shape = input_shape ) )
      model.add( Activation( 'relu' ) )
      model.add( Conv2D( 32, (3, 3) ) )
      model.add( Activation( 'relu') )
      model.add( MaxPooling2D( pool_size=(2, 2) ) )
      model.add( Dropout( 0.25 ) )

      model.add( Conv2D( 64, (3, 3), padding='same' ) )
      model.add( Activation( 'relu' ) )
      model.add( Conv2D( 64, (3, 3) ) )
      model.add( Activation( 'relu' ) )
      model.add( MaxPooling2D( pool_size=( 2, 2 ) ) )
      model.add( Dropout(0.25) )

      model.add( Flatten() )
      model.add( Dense( 512 ) )
      model.add( Activation( 'relu' ) )
      model.add( Dropout( 0.5 ) )
      model.add( Dense( 1 ) )
      model.add( Activation( 'relu' ) )

      optimizer = RMSprop(lr=0.00002, decay=6e-8)
      model.compile( loss='binary_crossentropy',
                      optimizer=optimizer,
                      metrics=['accuracy'] )
